{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from util import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1. read original word embedding\n",
    "word_vecs = read_vector('../data/fasttext.txt')\n",
    "embed_dim = len(word_vecs['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2. read resnet image feature\n",
    "img_feature = pickle.load(open('../img/img_mf.pkl', 'rb'))\n",
    "img_feature.update(pickle.load(open('../img/img_mm.pkl', 'rb')))\n",
    "img_feature.update(pickle.load(open('../img/img_ms.pkl', 'rb')))\n",
    "img_feature.update(pickle.load(open('../img/img_sp.pkl', 'rb')))\n",
    "img_feature.update(pickle.load(open('../img/img_yh.pkl', 'rb')))\n",
    "img_feature.update(pickle.load(open('../img/img_fr.pkl', 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566641\n"
     ]
    }
   ],
   "source": [
    "#Step 3. read title image pair\n",
    "fin = open('../data/trn_title_img.txt', 'r')\n",
    "trn_data = [line.strip().split('\\t') for line in fin.readlines()]\n",
    "fin.close()\n",
    "titles = [d[0] for d in trn_data]\n",
    "img_names = [d[1] for d in trn_data]\n",
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4. generate title embedding array\n",
    "title_vecs = []\n",
    "for t in titles:\n",
    "    t_vec = sent2vec(t, word_vecs)\n",
    "    title_vecs.append(t_vec)\n",
    "title_vecs = np.array(title_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5. generate img feature array\n",
    "img_vecs = []\n",
    "for img_name in img_names:\n",
    "    img_vecs.append(img_feature[img_name])\n",
    "del img_feature\n",
    "img_vecs = np.array(img_vecs)\n",
    "img_dim = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6. read test data, format: [label(0, 1), title 1, title 2]\n",
    "test_data = read_test_data('../data/test_data.txt')\n",
    "test_tv = []\n",
    "test_label = []\n",
    "for d in test_data:\n",
    "    test_label.append(float(d[0]))\n",
    "    test_tv.append(sent2vec(d[1], word_vecs))\n",
    "    test_tv.append(sent2vec(d[2], word_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7. set hyper parameters\n",
    "beta = 1e-2 #weight of orthogonal constraint\n",
    "gamma = 1e-7 #weight of variational dropout\n",
    "max_epoch = 10\n",
    "batch_size = 128\n",
    "steps = int(np.ceil(len(title_vecs)/batch_size))\n",
    "split_index = 150 #so image-informative section index is (0:150), image-uninformative is (150:300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Step 8. build data generator\n",
    "def generator():\n",
    "    index = -1\n",
    "    trn_idx = np.arange(len(img_vecs))\n",
    "    random.shuffle(trn_idx)\n",
    "    while True:\n",
    "        if index < len(title_vecs)-1:\n",
    "            index += 1\n",
    "        else:\n",
    "            index = 0\n",
    "            trn_idx = np.arange(len(img_vecs))\n",
    "            random.shuffle(trn_idx)\n",
    "        yield (title_vecs[trn_idx[index]], img_vecs[trn_idx[index]])\n",
    "        \n",
    "data = tf.data.Dataset.from_generator(generator, (tf.float32, tf.float32),(tf.TensorShape(300), tf.TensorShape(1000)))\n",
    "data = data.batch(batch_size)\n",
    "iterator = data.make_one_shot_iterator()\n",
    "batch_title_vecs, batch_img_vecs = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Step 9. build model\n",
    "#alpha stands for dropout rate in variational dropout\n",
    "log_alpha = tf.get_variable(\"log_alpha\", [embed_dim, 1], dtype=tf.float32, initializer=tf.initializers.random_normal(0,0.02))\n",
    "#theta is the model to construct image feature\n",
    "theta = tf.get_variable(\"theta\", [embed_dim, img_dim], dtype=tf.float32, initializer=tf.initializers.random_normal(0,0.1))\n",
    "#W is the transformation matrix\n",
    "W = tf.get_variable(\"W\", [embed_dim, embed_dim], initializer=tf.initializers.random_normal, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#orthogonal constraint\n",
    "WtW = tf.matmul(W, W, transpose_a=True)\n",
    "eye_matrix = tf.eye(embed_dim, dtype=tf.float32)\n",
    "orth_loss = tf.losses.mean_squared_error(labels=eye_matrix, predictions=WtW)\n",
    "\n",
    "#construct image with variational dropout\n",
    "dup_w = tf.identity(W)\n",
    "dis_title_vec = tf.matmul(dup_w, batch_title_vecs, transpose_b=True)\n",
    "assign_op1 = log_alpha.assign(tf.clip_by_value(log_alpha, -10, 10))\n",
    "head_drp = tf.identity(log_alpha[:split_index])\n",
    "tail_drp = tf.clip_by_value(tf.identity(log_alpha[split_index:]), 2.5, 10)\n",
    "assign_op2 = log_alpha.assign(tf.concat([head_drp, tail_drp],axis=0))\n",
    "#Strong disentanglement: manually bound the dropout rates for Image-Uninformative\n",
    "#Soft disentanglement: without control_dipendencies\n",
    "with tf.control_dependencies([assign_op1, assign_op2]):\n",
    "    alpha = tf.exp(log_alpha*0.5)\n",
    "    epsilon = tf.random_normal(shape=alpha.shape)\n",
    "    new_theta = theta*(1.0 + alpha*epsilon)\n",
    "    pred_img = tf.matmul(dis_title_vec, new_theta, transpose_a=True)\n",
    "    pred_loss = tf.losses.mean_squared_error(labels=batch_img_vecs, predictions=pred_img)\n",
    "\n",
    "#calculate KL-divergence\n",
    "k1, k2, k3 = 0.63576, 1.8732, 1.48695\n",
    "kl = k1 * tf.sigmoid(k2 + k3 * log_alpha) - 0.5 * tf.log1p(tf.exp(-log_alpha))\n",
    "kl_reg = - tf.reduce_sum(kl)\n",
    "\n",
    "#weighted loss\n",
    "losses = pred_loss + beta * orth_loss + gamma * kl_reg\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "train = optimizer.minimize(losses, var_list=[W, theta, log_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%\n",
      "Pred loss: 64.18037, Orth loss: 469.45328, acc: 0.7260\n",
      "Best result saved as W_7260.npy\n",
      "Epoch 2/10: 100%\n",
      "Pred loss: 0.00180, Orth loss: 228.39531, acc: 0.7256\n",
      "Epoch 3/10: 100%\n",
      "Pred loss: 0.00925, Orth loss: 49.89655, acc: 0.7240\n",
      "Epoch 4/10: 100%\n",
      "Pred loss: 0.00284, Orth loss: 4.72728, acc: 0.7292\n",
      "Best result saved as W_7292.npy\n",
      "Epoch 5/10: 100%\n",
      "Pred loss: 0.79547, Orth loss: 0.48377, acc: 0.7336\n",
      "Best result saved as W_7336.npy\n",
      "Epoch 6/10: 100%\n",
      "Pred loss: 0.00075, Orth loss: 0.45453, acc: 0.7344\n",
      "Best result saved as W_7344.npy\n",
      "Epoch 7/10: 100%\n",
      "Pred loss: 0.00092, Orth loss: 0.36854, acc: 0.7376\n",
      "Best result saved as W_7376.npy\n",
      "Epoch 8/10: 100%\n",
      "Pred loss: 0.00101, Orth loss: 0.14602, acc: 0.7500\n",
      "Best result saved as W_7500.npy\n",
      "Epoch 9/10: 100%\n",
      "Pred loss: 0.66359, Orth loss: 0.02882, acc: 0.7500\n",
      "Epoch 10/10: 100%\n",
      "Pred loss: 0.00116, Orth loss: 0.02608, acc: 0.7512\n",
      "Best result saved as W_7512.npy\n"
     ]
    }
   ],
   "source": [
    "#Step 10. train and evaluate\n",
    "with tf.Session() as sess:\n",
    "    best_acc = 0\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(max_epoch):\n",
    "        orth_losses = []\n",
    "        pred_losses = []\n",
    "        for j in range(steps):\n",
    "            #beta = min(1.0 * i / 30, 1.0) #anther option is to gradually increase the weight of orthogonal constraint\n",
    "            _, orth_loss_val, pred_loss_val = sess.run((train, orth_loss, pred_loss))\n",
    "            orth_losses.append(orth_loss_val)\n",
    "            pred_losses.append(pred_loss_val)\n",
    "            print(\"\\rEpoch {}/{}: {:.0%}\".format(i+1, max_epoch, j/steps), end='')\n",
    "        #validatioin and save the best result\n",
    "        log_alpha_val, W_val = sess.run((log_alpha, W))\n",
    "        dis_test_tv = np.matmul(W_val, np.array(test_tv).T).T\n",
    "        val_acc = evaluate(W_val, dis_test_tv, test_label)\n",
    "        print(\"\\nPred loss: {:.5f}, Orth loss: {:.5f}, acc: {:.4f}\".format(np.mean(pred_losses), np.mean(orth_losses), val_acc))\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            dropout = np.exp(log_alpha_val)/(1+np.exp(log_alpha_val))\n",
    "            np.save(\"../output/W_{}.npy\".format(int(best_acc*10000)), W_val)\n",
    "            np.save(\"../output/dropout_{}.npy\".format(int(best_acc*10000)), dropout)\n",
    "            print(\"Best result saved as W_{}.npy\".format(int(best_acc*10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37191665\n",
      "0.92415243\n"
     ]
    }
   ],
   "source": [
    "#Step 11. check dropout rate\n",
    "dropout = np.load(\"../output/dropout_7512.npy\")\n",
    "print(np.mean(dropout[:150]))\n",
    "print(np.mean(dropout[150:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best T: 0.69\n",
      "Accuracy: 75.12%\n",
      "Precision: 76.26%\n",
      "Recall: 79.48%\n",
      "F1-score: 77.83%\n"
     ]
    }
   ],
   "source": [
    "#Step 12. save evaluation result\n",
    "W_val = np.load(\"../output/W_7512.npy\")\n",
    "dis_test_tv = np.matmul(W_val, np.array(test_tv).T).T\n",
    "acc = evaluate(W_val, dis_test_tv, test_label, printout=True, savepath='../output/eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best T: 0.70\n",
      "Accuracy: 73.48%\n",
      "Precision: 75.27%\n",
      "Recall: 77.07%\n",
      "F1-score: 76.16%\n"
     ]
    }
   ],
   "source": [
    "#evaluate Image-Informative\n",
    "acc_d1 = evaluate(W_val, dis_test_tv[:, :split_index], test_label, printout=True, savepath='../output/eval_d1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best T: 0.68\n",
      "Accuracy: 75.24%\n",
      "Precision: 75.73%\n",
      "Recall: 80.86%\n",
      "F1-score: 78.21%\n"
     ]
    }
   ],
   "source": [
    "#evaluate Image-Uninformative\n",
    "acc_d2 = evaluate(W_val, dis_test_tv[:, split_index:], test_label, printout=True, savepath='../output/eval_d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 13. search for nearest neighbors\n",
    "trans_word_vecs = {}\n",
    "for w in word_vecs:\n",
    "    wv = np.matmul(W_val, np.array(word_vecs[w]).T).T\n",
    "    trans_word_vecs[w] = wv\n",
    "dis_title_vecs = np.matmul(W_val, title_vecs.T).T\n",
    "vecs = [dis_title_vecs, dis_title_vecs[:, :split_index], dis_title_vecs[:, split_index:]]\n",
    "nbrs = []\n",
    "for v in vecs:\n",
    "    nbr = NearestNeighbors(n_neighbors=16, metric='cosine').fit(v)\n",
    "    nbrs.append(nbr)\n",
    "def search(key_words):\n",
    "    v = sent2vec(key_words, trans_word_vecs)\n",
    "    _, idxs0 = nbrs[0].kneighbors([v])\n",
    "    _, idxs1 = nbrs[1].kneighbors([v[:split_index]])\n",
    "    _, idxs2 = nbrs[2].kneighbors([v[split_index:]])\n",
    "    diff1 = set(idxs1[0]).difference(set(idxs2[0]))\n",
    "    diff2 = set(idxs2[0]).difference(set(idxs1[0]))\n",
    "    print('====DisFastText====')\n",
    "    for i in idxs0[0]:\n",
    "        print(titles[i])\n",
    "    print('\\n')\n",
    "    print('====DisFastText Image-Imformative====')\n",
    "    for i in diff1:\n",
    "        print(titles[i])\n",
    "    print('\\n')\n",
    "    print('====DisFastText Image-Uinformative====')\n",
    "    for i in diff2:\n",
    "        print(titles[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【clinique 倩碧】水磁場72h超循環保濕凝膠(50ml)\n",
      "黏樂趣 nelo 卡通造形重複貼掛勾組(駝羊)\n",
      "【andzen】日系風格香氛負離子水氧機az-1168(來自澳洲單方精油10mlx3瓶)\n",
      "【samsung】galaxy j5 / grand prime g530/g531/g530y專用 原廠電池(裸裝)\n",
      "lee 牛仔褲 726中腰舒適刷色小直筒牛仔褲/ur 男款 深藍色\n",
      "大象生活館 附發票 大家源1.5l美食養生鍋tcy-2745快煮壺/電茶壺/中藥壺/藥膳壺/花茶壺/優格機/美食鍋/溫酒\n",
      "epson l3110 三合一 連續供墨複合機\n",
      "【維維樂】舒必克 蜂膠兒童喉片-葡萄 （30顆/盒） 6盒 專為兒童\n",
      "【大霹靂】samsung galaxy s8+ / s8 plus 布袋戲彩繪磁力皮套(紅塵雪)\n",
      "【ihouse】sgs 防潮抗蟲蛀緩衝塑鋼加高五門三抽半開放式置物鞋櫃 寬97深33.5高180cm\n",
      "adata威剛 sd600 512gb(紅或黑) usb3.1 外接式ssd行動硬碟\n",
      "【阿舍食堂】外省乾麵「油蔥」(5包入475g)x4袋\n",
      "【太極數位】cy hp gk200 有線機械式電競鍵盤 輕巧 方便 耐用\n",
      "for nikon en-el11/12 智慧型充電器(micro usb輸入充電)\n",
      "[106 美國直購] bissell 1095 清新芳香片(8入)spring breeze steam mop fragrance discs\n",
      "日本threeway桌上型12位計算機 twc-20\n",
      "【尚朋堂】3人份養生不鏽鋼電鍋(不鏽鋼配件)(ssc-007)\n",
      "push! 戶外休閒登山用品195g高強度航空碳纖維coolmax手腕帶3ls三節式登山杖(2入)p67\n",
      "【元山】智慧冰溫熱落地式飲水機ys-8211rwsab\n",
      "新竹【超人3c】kinyo 耐嘉 us-301 2.0多媒體音箱 us301 金屬鐵網 桌上型電腦/筆電/手機/平板\n",
      "【登記送dc扇+好禮3選1★富士通】4.5坪優級m系列r32變頻冷專分離式(ascg028cmtb/aocg028cmtb)\n",
      "【day&day】雙層置物架-窄版(st3268-2s)\n",
      "air cell-05 韓國7cm雙鉤型減壓背帶(背包專用)\n",
      "【shiseido 資生堂東京櫃】怡麗絲爾極奢潤膠原柔膚水 170ml〈百貨公司貨〉\n",
      "【viewsonic 優派】viewstylus 微軟觸控手寫筆(acp301經典黑)\n",
      "【華碩 asus】ws860t 高階繪圖工作站（e5-1630v4 32g 256g ssd+2tb quadro p4000 8gb 專業繪圖卡 win7專業版 三年保固）(阿福3c)\n",
      "【albert 艾柏】艾柏 正三線抗菌天絲3.5尺單人乳膠獨立筒床墊(3.5x6.2尺)\n",
      "secufirst wp-g01sc 旋轉hd無線網路攝影機\n",
      "【loreal paris 巴黎萊雅】活力緊緻 激光煥膚深夜修護面膜晚霜(50ml)\n",
      "創見 ssd370s 370s 128gb sata3 2.5吋 ssd 固態硬碟 鋁殼版 全新品開發票\n",
      "【獨家送dc扇★sanlux 台灣三洋】528公升一級能效直流變頻三門冰箱(sr-c528cvg)\n",
      "【伊莉貝特】雙人特大床墊套 183*214*30cm(防蹣寢具純棉)\n",
      "【日本珍珠金屬】花漾金屬廚具收納籃\n",
      "【書寶二手書t7／政治_hfm】常識-一個台灣人最好知道的事_施明德\n",
      "已改全區pioneer先鋒 倍頻hdmi dvd播放機(dv-3052v) dv-3052 送hdmi 線\n",
      "msi微星 ge75 8sg-089tw 17.3吋電競筆電(i7-8750h/16g/512g+1t/rtx2080-8g/win10)\n",
      "【成功】s5141涼感可調式護腕(單入)\n",
      "【sampo 聲寶】精品變頻單冷分離式一對一冷氣約15坪(au-qc93d/am-qc93d)\n",
      "作客遊台灣-台東日日有意思：開始在太平洋畔慢旅行\n",
      "【3m】無痕防水收納-廚房多功能排鉤\n",
      "3m 保膚清爽乳液 500ml【新高橋藥妝】\n",
      "【sun color】悠然-紫 雙人100%天絲四件式兩用被套床包組\n",
      "【5th street】男牛仔復古藍修身褲-灰藍色\n",
      "24期零利率 canon ts-e 90mm f2.8l macro 標準移軸鏡頭 公司貨\n",
      "【good life得意人生】q10複方膠囊(60粒)[9141]\n",
      "tefal法國特福 超導不鏽鋼系列32cm小炒鍋\n",
      "【fitness】100%純小羊毛雙人加大被立體邊8*7(加重版4.2公斤)\n",
      "【shuter樹德】大a4七層單排落地雪白資料櫃(7高抽)\n",
      "【logitech 羅技】g310 精簡型機械遊戲鍵盤\n",
      "【bobbi brown 芭比布朗】全效持久飾底乳霜spf 25 pa++ 7ml x 2入(櫃姐激推品)\n",
      "babybjorn 便器椅\n",
      "whirlpool惠而浦10l節能除濕機 wdee20w\n",
      "【馬告麻吉】筷子肉條12件組(蜜汁/泰式/黑胡椒/蒜香)\n",
      "【安親】草本成人紙尿褲 m號超值經濟型(16+1片x6包/箱)\n",
      "【at home】現代簡約7.3尺三件組合衣櫃(四抽+三抽+開放書櫃/哈佛)\n",
      "【生活采家】台灣製304不鏽鋼廚房經典跨海大橋伸縮瀝水架(#27007)\n",
      "【panasonic國際牌】二合一蒸氣電熨斗ni-fs470-k\n",
      "【v.team】假兩件修身韻律運動褲裙(2款4色)\n",
      "【kaspersky 卡巴斯基】全方位安全軟體2019 1台裝置1年授權(kts20191d1y)\n",
      "【sunflower三花】三花急暖輕著女保暖衣.發熱衣(2件組)\n",
      "(二手書)歡迎光臨丹妮婊姐星球：專屬於二百五loser的心靈雞湯\n",
      "【百翠氏】岩薔薇絕對精油原精3% -10ml(新鮮香脂木本草本和略辣花香)\n",
      "【迪士尼 授權正版】iphone 6 4.7吋 金蒔繪雙料透明手機軟殼(童話版)\n",
      "meike n2 美科電子快門線for nikon mc-dc1 公司貨\n",
      "【garmin】forerunner 935(黑色) 腕式心率 全方位鐵人運動錶 【西瓜籽特惠專賣】(穿戴式裝置 /心跳 /心率 /路跑 /慢跑)\n",
      "日本hoppetta六層紗綿羊防踢背心 (新生兒~3歲)\n",
      "【愛的世界】superkids 小狗訓練師系列純棉公主領長袖上衣/10~12歲-台灣製-\n",
      "【lotte】木糖醇+2無糖口香糖-清涼薄荷(35g)/袋\n",
      "【wusthof 三叉】cleaver 16cm剁刀(厚實刀身 精準下刀)\n",
      "magic 省電粉嫩蝸牛家族 tm-241c 倒數計時器 充電時間到自動關閉電源\n",
      "archgon亞齊慷 里約森巴風-鵝絨黃 濾藍光眼鏡 (gl-b107-y)\n",
      "canon pixma g3010 原廠大供墨複合機 /適用 gi-790bk/gi-790c/gi-790m/gi-790y\n",
      "flash bow 鋒寶 橫式 fb-3958 led電腦萬年曆 電子鐘 ~農曆年有生肖 二十四節氣\n",
      "【格藍傢飾】水立方涼感彈性沙發套1人座/2人座/3人座/1+2+3人座\n",
      "nike 耐吉 nike free rn 2018 (gs)  慢跑鞋 ah3451003 *女\n",
      "【tefal 特福】mambo玻璃內膽長效保溫壺1l+不鏽鋼隨行保溫瓶-700ml海軍藍(德國emsa)\n",
      "神祕埃及\n",
      "快速出貨↘72折《台塑生醫》龜鹿四珍養生液(50ml*14瓶/盒) 6盒/組\n",
      "【bernice】夏爾德實木吧台椅/吧檯椅/高腳椅(矮-二入組合)\n",
      "【海夫x舒背爾】九國專利 可調式 護腰靠墊(商務版)\n",
      "2018－19世界名錶年鑑（硬皮精裝本）\n",
      "【美吾髮】卡樂芙優質染髮霜(亞麻綠)\n",
      "【a-one】台灣製 加大床包枕套三件組 - 唯你 雪紡棉磨毛加工處理\n",
      "【葛瑞絲愛保養】理膚寶水公司貨 安得利清爽極效夏卡防曬液 spf50+ 50ml\n",
      "【yamaha 山葉古典吉他】4/4標準桶身 / 含琴袋 公司貨(cgs104a)\n",
      "【dr.wu 達爾膚】潤透光美白眼部精華液15ml\n",
      "【bocelli】marais瑪黑風尚高背辦公椅(義大利牛皮)經典黑\n",
      "法壓壺 家用玻璃 咖啡壺 沖茶器 法式濾壓壺350ml dr8108\n",
      "【i-rocks】irk65mn機械式電競鍵盤-德國cherry茶軸\n",
      "yamaha ysp-1600 5.1聲道無線家庭劇院聲霸 可連結 wifi 藍芽 musiccast\n",
      "【edwin】江戶勝印繡玩味厚長袖t恤-男款(米白色)\n",
      "【rose】外星人造型長抱枕(大)\n",
      "【瓏山林蘇澳冷熱泉度假飯店】湯屋2h+下午茶雙人券(10張)\n",
      "samsung三星 c43j890dke 43型 va曲面(32:10)電競寬液晶螢幕\n",
      "國際牌★panasonic★台灣松下★奈米水離子美顏器《eh-sa43》\n",
      "lynx - 美國山貓進口牛皮系列14卡1照長夾-共2色\n",
      "oppo r17 pro 藍紫光 9h 鋼化玻璃膜 -超值3入組(手機 螢幕 防藍光 保護貼)\n",
      "【durance 朵昂思】薰衣草大地擴香補充瓶 250ml\n",
      "jlab epic air 真無線藍牙耳機\n",
      "--庫米--imak huawei nova 3e/p20 lite 羽翼ii水晶殼(pro版) 透明殼 全包覆保護殼\n"
     ]
    }
   ],
   "source": [
    "#sample some titles to search\n",
    "for i in range(100):\n",
    "    print(random.choice(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====DisFastText====\n",
      "英國vs沙宣 1200w國際電壓摺疊式負離子吹風機 vs912piw\n",
      "英國vs 沙宣1200w摺疊式吹風機 vs908pw\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子國際電壓吹風機(美髮超值組)\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子國際電壓吹風機(美髮超值組)\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子國際電壓吹風機(美髮超值組)\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子國際電壓吹風機(美髮超值組)\n",
      "英國vs 沙宣1300w陶瓷摺疊吹風機 vs157rdrw\n",
      "英國vs沙宣 1300w陶瓷摺疊吹風機 vs157rdrw\n",
      "【英國vs沙宣】1300w陶瓷摺疊吹風機 vs157rdrw\n",
      "【滿額贈.湊單再折】英國vs沙宣1200w摺疊式負離子深層滋潤吹風機(vs912piw 國際電壓)\n",
      "【滿額贈.湊單再折】英國vs沙宣1200w摺疊式負離子深層滋潤吹風機(vs912piw 國際電壓)\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "\n",
      "\n",
      "====DisFastText Image-Imformative====\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "達新牌 負離子吹風機 國際電壓 摺疊式(fd-2)\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "【滿額贈.湊單再折】英國vs沙宣1200w摺疊式負離子深層滋潤吹風機(vs912piw 國際電壓)\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "vs沙宣 環球電摺疊負離子吹風機vs912piw【愛買】\n",
      "vs沙宣 環球電摺疊負離子吹風機vs912piw【愛買】\n",
      "英國vs沙宣 復刻粉漾負離子折疊式吹風機 vs590piw\n",
      "\n",
      "\n",
      "====DisFastText Image-Uinformative====\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子吹風機(美髮超值組)\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子吹風機(美髮超值組)\n",
      "【英國vs沙宣】1300w陶瓷摺疊吹風機 vs157rdrw\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子吹風機(美髮超值組)\n",
      "英國vs沙宣 1300w陶瓷摺疊吹風機 vs157rdrw\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子國際電壓吹風機(美髮超值組)\n",
      "英國vs 沙宣1300w陶瓷摺疊吹風機 vs157rdrw\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子吹風機(美髮超值組)\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子吹風機(美髮超值組)\n",
      "【英國vs沙宣 x 法國babyliss】25mm鈦金陶瓷電棒捲+1200w負離子吹風機(美髮超值組)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search('英國vs沙宣 1200w國際電壓摺疊式負離子吹風機 vs912piw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====DisFastText====\n",
      "【naturehike】繽紛撞色款雙面可戴空頂遮陽帽/防曬帽(三色任選)\n",
      "【naturehike】繽紛撞色款雙面可戴空頂遮陽帽/防曬帽(三色任選)\n",
      "【naturehike】繽紛撞色款雙面可戴空頂遮陽帽/防曬帽 (紅粉色)\n",
      "【naturehike】繽紛撞色款雙面可戴空頂遮陽帽/防曬帽 (藍紫色)\n",
      "【naturehike】迷彩潮流款速乾透氣漁夫帽/遮陽帽/防曬帽 (五色任選)\n",
      "【naturehike】迷彩潮流款速乾透氣漁夫帽/遮陽帽/防曬帽(五色任選)\n",
      "【naturehike】迷彩潮流款速乾透氣漁夫帽/遮陽帽/防曬帽(五色任選)\n",
      "【fifi飛時尚】兩用造型蕾絲布面遮陽空頂帽 戶外防曬帽(8色任選)\n",
      "【cute ii lady】時尚亮片大帽檐空頂防曬遮陽帽(桃)\n",
      "【cute ii lady】時尚亮片大帽檐空頂防曬遮陽帽(桃)\n",
      "《條紋帽》條紋雙色雙面大帽沿遮陽帽 雙面可戴 帽沿可折 mz0668 防曬帽 漁夫帽 很輕巧\n",
      "《條紋帽》條紋雙色雙面大帽沿遮陽帽 雙面可戴 帽沿可折 mz0668 防曬帽 漁夫帽 很輕巧\n",
      "【cute ii lady】日本抗uv時尚涼感帽(8色任選)\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "\n",
      "\n",
      "====DisFastText Image-Imformative====\n",
      "【cute ii lady】時尚亮片大帽檐空頂防曬遮陽帽(粉)\n",
      "【cute ii lady】時尚亮片大帽檐空頂防曬遮陽帽(粉)\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "【lorensa蘿芮】抗uv蝴蝶結純棉透氣純色可折防風大帽簷防曬遮陽帽(卡其)\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "《條紋帽》條紋雙色雙面大帽沿遮陽帽 雙面可戴 帽沿可折 mz0668 防曬帽 漁夫帽 很輕巧\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "《條紋帽》條紋雙色雙面大帽沿遮陽帽 雙面可戴 帽沿可折 mz0668 防曬帽 漁夫帽 很輕巧\n",
      "【naturehike】upf50+時尚款折疊速乾鴨舌帽/遮陽帽/防曬帽(粉色)\n",
      "\n",
      "\n",
      "====DisFastText Image-Uinformative====\n",
      "sunlead 防曬寬帽緣。名模款抗uv時尚圓頂遮陽帽 (咖啡色)\n",
      "日本sunfamily 雙面雙色可折疊海軍風抗uv帽(黑/藍白條紋雙色)\n",
      "【挪威 actionfox】女新款 抗uv透氣超大帽簷雙面遮陽帽upf50+.大盤帽.圓盤帽/可雙面戴(631-4773 粉紅)\n",
      "【black yak】runner軍帽(黑色)\n",
      "【naturehike】迷彩潮流款速乾透氣漁夫帽/遮陽帽/防曬帽(五色任選)\n",
      "【naturehike】迷彩潮流款速乾透氣漁夫帽/遮陽帽/防曬帽(五色任選)\n",
      "日本sunfamily 雙面雙色可折疊海軍風抗uv帽(黑/藍白條紋雙色)\n",
      "【naturehike】迷彩潮流款速乾透氣漁夫帽/遮陽帽/防曬帽 (五色任選)\n",
      "【cute ii lady】日本抗uv時尚涼感帽(8色任選)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search('【naturehike】繽紛撞色款雙面可戴空頂遮陽帽/防曬帽(三色任選)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
